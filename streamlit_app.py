import streamlit as st
import pandas as pd
import requests
import json
import time
import re
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from io import StringIO

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¥ MedLLM Leaderboard",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        margin: -1rem -1rem 2rem -1rem;
        border-radius: 0 0 20px 20px;
    }
    
    .question-preview {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #28a745;
        margin: 1rem 0;
        font-size: 0.95rem;
        line-height: 1.6;
        white-space: pre-line;
    }
    
    .answer-highlight {
        background: #d4edda;
        border: 2px solid #c3e6cb;
        color: #155724;
        padding: 0.8rem;
        border-radius: 8px;
        font-weight: bold;
        text-align: center;
        margin: 1rem 0;
    }
    
    .rank-badge {
        display: inline-block;
        padding: 0.8rem;
        border-radius: 50%;
        text-align: center;
        font-weight: bold;
        min-width: 4rem;
        min-height: 4rem;
        line-height: 2.4rem;
        font-size: 1.1rem;
    }
    
    .rank-1 {
        background: linear-gradient(135deg, #ffd700 0%, #ffed4a 100%);
        color: #92400e;
        box-shadow: 0 4px 15px rgba(255, 215, 0, 0.4);
    }
    
    .rank-2 {
        background: linear-gradient(135deg, #c0c0c0 0%, #e5e7eb 100%);
        color: #374151;
        box-shadow: 0 4px 15px rgba(192, 192, 192, 0.4);
    }
    
    .rank-3 {
        background: linear-gradient(135deg, #cd7f32 0%, #d97706 100%);
        color: white;
        box-shadow: 0 4px 15px rgba(205, 127, 50, 0.4);
    }
    
    .performance-card {
        background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);
        padding: 1.5rem;
        border-radius: 12px;
        border-left: 4px solid #3b82f6;
        margin: 1rem 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'leaderboard_data' not in st.session_state:
    st.session_state.leaderboard_data = []
if 'current_dataset' not in st.session_state:
    st.session_state.current_dataset = None
if 'api_key' not in st.session_state:
    st.session_state.api_key = ""
if 'models_to_evaluate' not in st.session_state:
    st.session_state.models_to_evaluate = []

# HuggingFace API í´ë˜ìŠ¤ (ê°„ì†Œí™”ëœ í˜•ì‹ìš©)
class SimplifiedMedicalQA:
    def __init__(self, api_key):
        self.api_key = api_key
        self.api_url = 'https://api-inference.huggingface.co/models/'
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
        }
    
    def test_api_connection(self):
        """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.post(
                f"{self.api_url}gpt2",
                headers=self.headers,
                json={"inputs": "Test"},
                timeout=10
            )
            return response.status_code in [200, 503]
        except:
            return False
    
    def query_model(self, model_name, inputs, max_retries=3):
        """ëª¨ë¸ ì¿¼ë¦¬"""
        for attempt in range(max_retries):
            try:
                payload = {
                    "inputs": inputs,
                    "parameters": {
                        "max_new_tokens": 50,
                        "temperature": 0.1,
                        "do_sample": False,
                        "return_full_text": False,
                        "stop": ["\n\n", "ì§ˆë¬¸:", "Question:", "ë‹¤ìŒ"]
                    },
                    "options": {
                        "wait_for_model": True
                    }
                }
                
                response = requests.post(
                    f"{self.api_url}{model_name}",
                    headers=self.headers,
                    json=payload,
                    timeout=90
                )
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 503:
                    if attempt < max_retries - 1:
                        st.info(f"â³ ëª¨ë¸ ë¡œë”© ì¤‘... ({attempt + 1}/{max_retries})")
                        time.sleep(30)
                        continue
                    return {"error": "Model loading timeout"}
                else:
                    return {"error": f"API Error: {response.status_code}"}
                    
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    st.warning(f"â° íƒ€ì„ì•„ì›ƒ... ì¬ì‹œë„ ì¤‘ ({attempt + 1}/{max_retries})")
                    time.sleep(15)
                    continue
                return {"error": "Request timeout"}
            except Exception as e:
                return {"error": str(e)}
        
        return {"error": "Max retries exceeded"}
    
    def get_model_info(self, model_name):
        """ëª¨ë¸ ì •ë³´ í™•ì¸"""
        try:
            response = requests.get(
                f"https://huggingface.co/api/models/{model_name}",
                timeout=10
            )
            if response.status_code == 200:
                return response.json()
            return None
        except:
            return None
    
    def evaluate_qa_simple(self, model_name, questions, correct_answers, progress_callback=None):
        """ê°„ì†Œí™”ëœ QA í‰ê°€"""
        results = []
        total_questions = len(questions)
        
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = self.get_model_info(model_name)
        if not model_info:
            return [{"error": f"Model {model_name} not found"}]
        
        for i, (question, correct_answer) in enumerate(zip(questions, correct_answers)):
            if progress_callback:
                progress_callback(i + 1, total_questions)
            
            # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸ì— ì´ë¯¸ ì„ íƒì§€ê°€ í¬í•¨ë¨)
            prompt = f"""{question}

ì •ë‹µì„ A, B, C, D ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
ì •ë‹µ: """
            
            # API í˜¸ì¶œ
            response = self.query_model(model_name, prompt)
            
            predicted_answer = "No answer"
            error_msg = None
            raw_response_text = ""
            
            if isinstance(response, dict) and "error" in response:
                error_msg = response["error"]
                predicted_answer = "ERROR"
            elif isinstance(response, list) and len(response) > 0:
                if 'generated_text' in response[0]:
                    generated_text = response[0]['generated_text'].strip()
                    raw_response_text = generated_text
                    
                    # ë‹µë³€ ì¶”ì¶œ íŒ¨í„´ (ë” ê°„ë‹¨í•˜ê³  í™•ì‹¤í•œ íŒ¨í„´ë“¤)
                    patterns = [
                        r'^([ABCD])[\.\)\s]',      # ì²« ê¸€ìê°€ A, B, C, D
                        r'ì •ë‹µ[\s:]*([ABCD])',     # ì •ë‹µ: A
                        r'ë‹µ[\s:]*([ABCD])',       # ë‹µ: B  
                        r'([ABCD])[\s]*ë²ˆ',        # Aë²ˆ
                        r'ì„ íƒ[\s:]*([ABCD])',     # ì„ íƒ: C
                        r'\b([ABCD])\b'            # ë‹¨ë… A, B, C, D
                    ]
                    
                    for pattern in patterns:
                        match = re.search(pattern, generated_text, re.IGNORECASE)
                        if match:
                            predicted_answer = match.group(1).upper()
                            break
                    
                    # ë§ˆì§€ë§‰ ì‹œë„: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ A,B,C,D ì¤‘ ì²« ë²ˆì§¸ ì°¾ê¸°
                    if predicted_answer == "No answer":
                        abcd_matches = re.findall(r'[ABCD]', generated_text.upper())
                        if abcd_matches:
                            predicted_answer = abcd_matches[0]
            
            results.append({
                'question': question,
                'correct_answer': correct_answer,
                'predicted_answer': predicted_answer,
                'correct': predicted_answer == correct_answer,
                'raw_response': raw_response_text,
                'error': error_msg
            })
            
            # API ì œí•œ ê³ ë ¤
            time.sleep(3)
        
        return results
    
    def calculate_metrics(self, results):
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        valid_results = [r for r in results if not r.get('error')]
        correct = sum(1 for r in valid_results if r['correct'])
        total = len(valid_results)
        
        accuracy = correct / total if total > 0 else 0
        
        return {
            'accuracy': accuracy,
            'f1_score': accuracy,     # ê°„ì†Œí™”
            'precision': accuracy,    # ê°„ì†Œí™”
            'recall': accuracy,       # ê°„ì†Œí™”
            'total_questions': total,
            'correct_answers': correct,
            'error_count': len(results) - len(valid_results),
            'score': accuracy
        }

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ¥ MedLLM Leaderboard</h1>
    <p>ê°„ì†Œí™”ëœ í•œêµ­ì–´ ì˜ë£Œ QAë¡œ AI ëª¨ë¸ í‰ê°€ (Question + Answer í˜•ì‹)</p>
</div>
""", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # API í‚¤ ì„¤ì •
    st.subheader("ğŸ”‘ HuggingFace API Key")
    api_key = st.text_input(
        "API Key:",
        type="password",
        value=st.session_state.api_key,
        help="https://huggingface.co/settings/tokens"
    )
    
    if api_key != st.session_state.api_key:
        st.session_state.api_key = api_key
        if api_key:
            evaluator = SimplifiedMedicalQA(api_key)
            if evaluator.test_api_connection():
                st.success("âœ… API ì—°ê²° ì„±ê³µ!")
            else:
                st.error("âŒ API ì—°ê²° ì‹¤íŒ¨")
        else:
            st.warning("âš ï¸ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    st.divider()
    
    # ë°ì´í„°ì…‹ ì—…ë¡œë“œ
    st.subheader("ğŸ“Š ë°ì´í„°ì…‹ (ê°„ì†Œí™”)")
    
    # ìƒˆë¡œìš´ ê°„ì†Œí™”ëœ ìƒ˜í”Œ ìƒì„±
    simplified_sample = '''question,answer
"25ì„¸ ì—¬ì„±ì´ ì‹¬í•œ ë³µí†µê³¼ í•¨ê»˜ ì‘ê¸‰ì‹¤ì— ë‚´ì›í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ì›”ê²½ì€ 6ì£¼ ì „ì´ì—ˆê³ , ì†Œë³€ ì„ì‹ ë°˜ì‘ ê²€ì‚¬ëŠ” ì–‘ì„±ì…ë‹ˆë‹¤. í˜ˆì••ì€ 90/60 mmHg, ë§¥ë°•ì€ 110íšŒ/ë¶„ì…ë‹ˆë‹¤. ë³µë¶€ ê²€ì‚¬ì—ì„œ ì¢Œí•˜ë³µë¶€ì— ì••í†µì´ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì§„ë‹¨ì€?

A. ìê¶ì™¸ì„ì‹ 
B. ì¶©ìˆ˜ì—¼  
C. ë‚œì†Œë‚­ì¢… íŒŒì—´
D. ê³¨ë°˜ì—¼","A"
"45ì„¸ ë‚¨ì„±ì´ ìš´ë™ í›„ ì‹¬í•œ í‰í†µì„ í˜¸ì†Œí•©ë‹ˆë‹¤. í†µì¦ì€ ì™¼ìª½ ì–´ê¹¨ì™€ íŒ”ë¡œ ë°©ì‚¬ë˜ë©°, ì‹ì€ë•€ì„ í˜ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ì‹¬ì „ë„ì—ì„œ V2-V4 ìœ ë„ì—ì„œ ST ë¶„ì ˆ ìƒìŠ¹ì´ ê´€ì°°ë©ë‹ˆë‹¤. ê°€ì¥ ì ì ˆí•œ ì´ˆê¸° ì¹˜ë£ŒëŠ”?

A. ì•„ìŠ¤í”¼ë¦° íˆ¬ì—¬
B. ë‹ˆíŠ¸ë¡œê¸€ë¦¬ì„¸ë¦° ì„¤í•˜ì •
C. ì‚°ì†Œ ê³µê¸‰
D. ì¦‰ì‹œ ì‹¬ë„ììˆ ","D"
"60ì„¸ ë‚¨ì„±ì´ 6ê°œì›”ê°„ ì²´ì¤‘ê°ì†Œì™€ í™©ë‹¬ì„ ë³´ì…ë‹ˆë‹¤. ë³µë¶€ CTì—ì„œ ì·Œì¥ ë‘ë¶€ì— ì¢…ê´´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. CA 19-9 ìˆ˜ì¹˜ê°€ í˜„ì €íˆ ìƒìŠ¹ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì§„ë‹¨ì€?

A. ì·Œì¥ì—¼
B. ì·Œì¥ì•”
C. ë‹´ì„ì¦  
D. ê°„ê²½í™”","B"'''
    
    st.download_button(
        label="ğŸ“¥ ê°„ì†Œí™”ëœ ì˜ë£Œ QA ìƒ˜í”Œ",
        data=simplified_sample,
        file_name="simplified_medical_qa.csv",
        mime="text/csv"
    )
    
    st.info("""
    **ìƒˆë¡œìš´ CSV í˜•ì‹:**
    - `question`: ì§ˆë¬¸ + ì„ íƒì§€ í¬í•¨
    - `answer`: ì •ë‹µ (A, B, C, D)
    """)
    
    uploaded_file = st.file_uploader(
        "CSV íŒŒì¼ ì—…ë¡œë“œ:",
        type=['csv'],
        help="question, answer ì»¬ëŸ¼ë§Œ í•„ìš”"
    )
    
    if uploaded_file is not None:
        try:
            stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
            df = pd.read_csv(stringio)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            if 'question' not in df.columns or 'answer' not in df.columns:
                st.error("âŒ 'question', 'answer' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤")
            else:
                # ë°ì´í„° ì •ì œ
                df_clean = df.dropna(subset=['question', 'answer'])
                
                if len(df_clean) == 0:
                    st.error("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                else:
                    st.session_state.current_dataset = df_clean
                    st.success(f"âœ… {len(df_clean)}ê°œ ë¬¸ì œ ë¡œë“œ!")
                    
                    # ë‹µë³€ ë¶„í¬
                    answer_dist = df_clean['answer'].value_counts()
                    st.write("**ì •ë‹µ ë¶„í¬:**")
                    for ans, count in answer_dist.items():
                        st.write(f"- {ans}: {count}ê°œ")
        
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {str(e)}")
    
    st.divider()
    
    # ëª¨ë¸ ì¶”ê°€
    st.subheader("ğŸ¤– AI ëª¨ë¸")
    
    # ì¶”ì²œ ëª¨ë¸
    if st.session_state.api_key:
        with st.expander("ğŸŒŸ ì¶”ì²œ ëª¨ë¸"):
            models = [
                ("google/gemma-2b-it", "Gemma 2B", "â­â­â­"),
                ("microsoft/BioGPT", "BioGPT", "â­â­â­"),
                ("microsoft/DialoGPT-medium", "DialoGPT-M", "â­â­"),
                ("distilgpt2", "DistilGPT-2", "â­â­"),
                ("gpt2", "GPT-2", "â­")
            ]
            
            for model_id, name, rating in models:
                col1, col2, col3 = st.columns([2, 1, 1])
                with col1:
                    st.write(f"**{name}** {rating}")
                    st.caption(model_id)
                with col2:
                    pass
                with col3:
                    if st.button("â•", key=f"add_{model_id}"):
                        if model_id not in [m['id'] for m in st.session_state.models_to_evaluate]:
                            st.session_state.models_to_evaluate.append({
                                'id': model_id,
                                'name': name
                            })
                            st.success(f"âœ… {name} ì¶”ê°€!")
                            st.rerun()
    
    # ì»¤ìŠ¤í…€ ëª¨ë¸ ì¶”ê°€
    with st.form("add_model"):
        model_id = st.text_input("ëª¨ë¸ ID:", placeholder="google/gemma-2b-it")
        model_name = st.text_input("í‘œì‹œ ì´ë¦„:", placeholder="ì»¤ìŠ¤í…€ ì´ë¦„")
        
        if st.form_submit_button("â• ëª¨ë¸ ì¶”ê°€"):
            if model_id:
                name = model_name if model_name else model_id.split('/')[-1]
                if model_id not in [m['id'] for m in st.session_state.models_to_evaluate]:
                    st.session_state.models_to_evaluate.append({
                        'id': model_id,
                        'name': name
                    })
                    st.success(f"âœ… {name} ì¶”ê°€!")
                    st.rerun()
    
    # ì¶”ê°€ëœ ëª¨ë¸ ëª©ë¡
    if st.session_state.models_to_evaluate:
        st.subheader("ğŸ“ í‰ê°€ ëŒ€ê¸°")
        for i, model in enumerate(st.session_state.models_to_evaluate):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"ğŸ¤– {model['name']}")
                st.caption(model['id'])
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{i}"):
                    st.session_state.models_to_evaluate.pop(i)
                    st.rerun()

# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([3, 2])

with col1:
    st.header("ğŸ† ì„±ëŠ¥ ë¦¬ë”ë³´ë“œ")
    
    if not st.session_state.leaderboard_data:
        st.info("""
        ğŸ¯ **ê°„ì†Œí™”ëœ ì˜ë£Œ QA í‰ê°€**
        
        1. API Key ì„¤ì • âœ¨
        2. question, answer í˜•ì‹ CSV ì—…ë¡œë“œ ğŸ“Š  
        3. AI ëª¨ë¸ ì¶”ê°€ ğŸ¤–
        4. í‰ê°€ ì‹œì‘! ğŸš€
        """)
    else:
        df_results = pd.DataFrame(st.session_state.leaderboard_data)
        df_results = df_results.sort_values('score', ascending=False).reset_index(drop=True)
        
        for idx, row in df_results.iterrows():
            rank = idx + 1
            
            # ìˆœìœ„ ë°°ì§€
            if rank == 1:
                badge = "ğŸ¥‡"
                badge_class = "rank-1"
            elif rank == 2:
                badge = "ğŸ¥ˆ"
                badge_class = "rank-2"
            elif rank == 3:
                badge = "ğŸ¥‰"
                badge_class = "rank-3"
            else:
                badge = f"#{rank}"
                badge_class = ""
            
            # ì„±ëŠ¥ ì¹´ë“œ
            with st.container():
                st.markdown("---")
                col_rank, col_info, col_metrics = st.columns([1, 2, 2])
                
                with col_rank:
                    st.markdown(f'<div class="rank-badge {badge_class}">{badge}</div>', unsafe_allow_html=True)
                
                with col_info:
                    st.subheader(f"ğŸ¤– {row['modelName']}")
                    st.caption(f"ğŸ“ {row['modelId']}")
                    st.caption(f"ğŸ“… {row['evaluatedAt']}")
                    
                    # ì„±ëŠ¥ ë“±ê¸‰
                    accuracy = row['accuracy'] * 100
                    if accuracy >= 80:
                        grade = "ğŸŸ¢ Aê¸‰ (ìš°ìˆ˜)"
                    elif accuracy >= 60:
                        grade = "ğŸŸ¡ Bê¸‰ (ì–‘í˜¸)"
                    elif accuracy >= 40:
                        grade = "ğŸŸ  Cê¸‰ (ë³´í†µ)"
                    else:
                        grade = "ğŸ”´ Dê¸‰ (ê°œì„ í•„ìš”)"
                    
                    st.markdown(f"**{grade}**")
                
                with col_metrics:
                    st.metric("ì •í™•ë„", f"{row['accuracy']:.1%}")
                    st.metric("ì •ë‹µ/ì „ì²´", f"{row['correct_answers']}/{row['total_questions']}")
                    
                    if row.get('error_count', 0) > 0:
                        st.warning(f"âš ï¸ {row['error_count']}ê°œ ì˜¤ë¥˜")
                
                # ì‚­ì œ ë²„íŠ¼
                if st.button(f"ğŸ—‘ï¸ {row['modelName']} ì‚­ì œ", key=f"delete_{idx}"):
                    st.session_state.leaderboard_data = [
                        item for item in st.session_state.leaderboard_data 
                        if item['modelId'] != row['modelId']
                    ]
                    st.rerun()

with col2:
    st.header("ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´")
    
    if st.session_state.current_dataset is not None:
        df = st.session_state.current_dataset
        st.success(f"âœ… **{len(df)}ê°œ** ë¬¸ì œ ë¡œë“œë¨")
        
        # ë‹µë³€ ë¶„í¬ ì°¨íŠ¸
        with st.expander("ğŸ“ˆ ì •ë‹µ ë¶„í¬", expanded=True):
            answer_counts = df['answer'].value_counts()
            fig = px.pie(
                values=answer_counts.values,
                names=answer_counts.index,
                title="ì •ë‹µ ë¶„í¬"
            )
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
        
        # ë¬¸ì œ ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ‘€ ë¬¸ì œ ë¯¸ë¦¬ë³´ê¸°", expanded=True):
            if len(df) > 0:
                sample_idx = st.selectbox(
                    "ë¬¸ì œ ì„ íƒ:",
                    range(len(df)),
                    format_func=lambda x: f"ë¬¸ì œ {x+1}"
                )
                
                sample = df.iloc[sample_idx]
                
                st.markdown(f'<div class="question-preview">{sample["question"]}</div>', unsafe_allow_html=True)
                st.markdown(f'<div class="answer-highlight">ì •ë‹µ: {sample["answer"]}</div>', unsafe_allow_html=True)
    else:
        st.warning("âš ï¸ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
        st.info("""
        **í•„ìš”í•œ í˜•ì‹:**
        ```
        question,answer
        "ì§ˆë¬¸ + ì„ íƒì§€","A"
        ```
        """)
    
    st.divider()
    
    # í‰ê°€ ì‹¤í–‰
    st.header("ğŸš€ í‰ê°€ ì‹¤í–‰")
    
    can_run = (
        st.session_state.api_key and 
        st.session_state.current_dataset is not None and 
        st.session_state.models_to_evaluate
    )
    
    if not can_run:
        missing = []
        if not st.session_state.api_key:
            missing.append("ğŸ”‘ API Key")
        if st.session_state.current_dataset is None:
            missing.append("ğŸ“Š ë°ì´í„°ì…‹")
        if not st.session_state.models_to_evaluate:
            missing.append("ğŸ¤– ëª¨ë¸")
        
        st.error("**í•„ìš”í•œ í•­ëª©:**\n" + "\n".join([f"- {item}" for item in missing]))
    else:
        num_questions = len(st.session_state.current_dataset)
        num_models = len(st.session_state.models_to_evaluate)
        estimated_minutes = (num_questions * num_models * 3) // 60
        
        st.success("âœ… ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ!")
        st.info(f"â±ï¸ ì˜ˆìƒ ì‹œê°„: ì•½ {estimated_minutes}ë¶„")
        st.caption(f"ğŸ“Š {num_models}ê°œ ëª¨ë¸ Ã— {num_questions}ê°œ ë¬¸ì œ")
    
    if st.button("ğŸ¯ í‰ê°€ ì‹œì‘!", disabled=not can_run, type="primary", use_container_width=True):
        if can_run:
            evaluator = SimplifiedMedicalQA(st.session_state.api_key)
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            current_model_info = st.empty()
            
            # ë°ì´í„° ì¤€ë¹„
            df = st.session_state.current_dataset
            questions = df['question'].tolist()
            correct_answers = df['answer'].tolist()
            
            total_models = len(st.session_state.models_to_evaluate)
            
            try:
                for model_idx, model in enumerate(st.session_state.models_to_evaluate):
                    current_model_info.info(f"ğŸ”„ **{model['name']}** í‰ê°€ ì¤‘... ({model_idx + 1}/{total_models})")
                    
                    # ëª¨ë¸ ì •ë³´ í™•ì¸
                    model_info = evaluator.get_model_info(model['id'])
                    if not model_info:
                        st.error(f"âŒ '{model['id']}' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        continue
                    
                    def progress_callback(current, total):
                        overall_progress = (model_idx + current/total) / total_models
                        progress_bar.progress(overall_progress)
                        status_text.text(f"ğŸ“ ë¬¸ì œ {current}/{total} ì²˜ë¦¬ ì¤‘...")
                    
                    # í‰ê°€ ì‹¤í–‰
                    results = evaluator.evaluate_qa_simple(
                        model['id'],
                        questions,
                        correct_answers,
                        progress_callback
                    )
                    
                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    metrics = evaluator.calculate_metrics(results)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    accuracy_pct = metrics['accuracy'] * 100
                    if accuracy_pct >= 70:
                        result_emoji = "ğŸ‰"
                    elif accuracy_pct >= 50:
                        result_emoji = "ğŸ‘"
                    else:
                        result_emoji = "ğŸ’ª"
                    
                    st.success(f"{result_emoji} **{model['name']}** ì™„ë£Œ: {accuracy_pct:.1f}% ({metrics['correct_answers']}/{metrics['total_questions']})")
                    
                    # ê²°ê³¼ ì €ì¥
                    new_entry = {
                        'modelId': model['id'],
                        'modelName': model['name'],
                        'accuracy': metrics['accuracy'],
                        'f1_score': metrics['f1_score'],
                        'precision': metrics['precision'],
                        'recall': metrics['recall'],
                        'score': metrics['score'],
                        'total_questions': metrics['total_questions'],
                        'correct_answers': metrics['correct_answers'],
                        'error_count': metrics['error_count'],
                        'evaluatedAt': datetime.now().strftime('%Y-%m-%d %H:%M'),
                        'pipeline_tag': model_info.get('pipeline_tag', 'unknown')
                    }
                    
                    # ê¸°ì¡´ ê²°ê³¼ ì œê±° í›„ ìƒˆ ê²°ê³¼ ì¶”ê°€
                    st.session_state.leaderboard_data = [
                        item for item in st.session_state.leaderboard_data 
                        if item['modelId'] != model['id']
                    ]
                    st.session_state.leaderboard_data.append(new_entry)
                
                # í‰ê°€ ì™„ë£Œ
                st.session_state.models_to_evaluate = []
                progress_bar.progress(1.0)
                status_text.text("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
                current_model_info.success(f"ğŸ‰ **{total_models}ê°œ ëª¨ë¸** í‰ê°€ ì™„ë£Œ!")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í‘œì‹œ
                if st.session_state.leaderboard_data:
                    best_model = max(st.session_state.leaderboard_data, key=lambda x: x['score'])
                    st.balloons()
                    st.info(f"ğŸ† **ìµœê³  ì„±ëŠ¥:** {best_model['modelName']} ({best_model['accuracy']:.1%} ì •í™•ë„)")
                
                st.rerun()
                
            except Exception as e:
                st.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                status_text.text("âŒ í‰ê°€ ì‹¤íŒ¨")

# ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸
if st.session_state.leaderboard_data:
    st.header("ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„")
    
    df_analysis = pd.DataFrame(st.session_state.leaderboard_data)
    
    # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ë“¤
    col1, col2 = st.columns(2)
    
    with col1:
        # ì •í™•ë„ ë°” ì°¨íŠ¸
        fig1 = px.bar(
            df_analysis.sort_values('accuracy', ascending=True),
            x='accuracy',
            y='modelName',
            orientation='h',
            title="ğŸ¯ ëª¨ë¸ë³„ ì •í™•ë„ ë¹„êµ",
            labels={'accuracy': 'ì •í™•ë„', 'modelName': 'ëª¨ë¸'},
            color='accuracy',
            color_continuous_scale='RdYlGn',
            text='accuracy'
        )
        fig1.update_traces(texttemplate='%{text:.1%}', textposition='outside')
        fig1.update_layout(height=400, showlegend=False)
        st.plotly_chart(fig1, use_container_width=True)
    
    with col2:
        # ì •ë‹µ ìˆ˜ ì‚°ì ë„
        fig2 = px.scatter(
            df_analysis,
            x='total_questions',
            y='correct_answers',
            size='accuracy',
            color='accuracy',
            hover_name='modelName',
            title="ğŸ“Š ì •ë‹µ ìˆ˜ ë¶„í¬",
            labels={'total_questions': 'ì „ì²´ ë¬¸ì œ', 'correct_answers': 'ì •ë‹µ ìˆ˜'},
            color_continuous_scale='RdYlGn'
        )
        
        # ì™„ë²½í•œ ì„±ëŠ¥ ë¼ì¸ ì¶”ê°€
        if len(df_analysis) > 0:
            max_questions = df_analysis['total_questions'].max()
            fig2.add_shape(
                type="line",
                x0=0, y0=0, x1=max_questions, y1=max_questions,
                line=dict(color="gray", dash="dash"),
                name="ì™„ë²½í•œ ì„±ëŠ¥"
            )
        
        fig2.update_layout(height=400, showlegend=False)
        st.plotly_chart(fig2, use_container_width=True)
    
    # ì„±ëŠ¥ ë“±ê¸‰ë³„ ë¶„ë¥˜
    st.subheader("ğŸ… ì„±ëŠ¥ ë“±ê¸‰ ë¶„ë¥˜")
    
    # ë“±ê¸‰ë³„ë¡œ ê·¸ë£¹í™”
    grade_data = []
    for _, row in df_analysis.iterrows():
        accuracy = row['accuracy'] * 100
        if accuracy >= 80:
            grade = "Aê¸‰ (ìš°ìˆ˜)"
            color = "ğŸŸ¢"
        elif accuracy >= 60:
            grade = "Bê¸‰ (ì–‘í˜¸)"
            color = "ğŸŸ¡"
        elif accuracy >= 40:
            grade = "Cê¸‰ (ë³´í†µ)"
            color = "ğŸŸ "
        else:
            grade = "Dê¸‰ (ê°œì„ í•„ìš”)"
            color = "ğŸ”´"
        
        grade_data.append({
            'model': row['modelName'],
            'accuracy': accuracy,
            'grade': grade,
            'color': color,
            'correct': row['correct_answers'],
            'total': row['total_questions']
        })
    
    # ë“±ê¸‰ë³„ë¡œ ì •ë ¬ (Aê¸‰ë¶€í„°)
    grade_order = {"Aê¸‰ (ìš°ìˆ˜)": 4, "Bê¸‰ (ì–‘í˜¸)": 3, "Cê¸‰ (ë³´í†µ)": 2, "Dê¸‰ (ê°œì„ í•„ìš”)": 1}
    grade_data.sort(key=lambda x: (grade_order.get(x['grade'], 0), x['accuracy']), reverse=True)
    
    for item in grade_data:
        col1, col2, col3, col4 = st.columns([3, 2, 2, 1])
        
        with col1:
            st.write(f"**{item['model']}**")
        with col2:
            st.write(f"{item['color']} **{item['grade']}**")
        with col3:
            st.write(f"**{item['accuracy']:.1f}%** ({item['correct']}/{item['total']})")
        with col4:
            # ê°„ë‹¨í•œ ì„±ëŠ¥ ê²Œì´ì§€
            if item['accuracy'] >= 80:
                st.success("âœ¨")
            elif item['accuracy'] >= 60:
                st.info("ğŸ‘")
            elif item['accuracy'] >= 40:
                st.warning("ğŸ¤”")
            else:
                st.error("ğŸ’ª")
    
    # ìƒì„¸ í†µê³„
    st.subheader("ğŸ“‹ ìƒì„¸ í†µê³„")
    
    avg_accuracy = df_analysis['accuracy'].mean()
    best_accuracy = df_analysis['accuracy'].max()
    worst_accuracy = df_analysis['accuracy'].min()
    total_evaluated = len(df_analysis)
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("í‰ê·  ì •í™•ë„", f"{avg_accuracy:.1%}")
    with col2:
        st.metric("ìµœê³  ì •í™•ë„", f"{best_accuracy:.1%}")
    with col3:
        st.metric("ìµœì € ì •í™•ë„", f"{worst_accuracy:.1%}")
    with col4:
        st.metric("í‰ê°€ëœ ëª¨ë¸", f"{total_evaluated}ê°œ")
    
    # ì„±ëŠ¥ íˆìŠ¤í† ê·¸ë¨
    fig3 = px.histogram(
        df_analysis,
        x='accuracy',
        nbins=10,
        title="ğŸ“Š ì •í™•ë„ ë¶„í¬",
        labels={'accuracy': 'ì •í™•ë„', 'count': 'ëª¨ë¸ ìˆ˜'},
        color_discrete_sequence=['#3b82f6']
    )
    fig3.update_layout(height=300)
    st.plotly_chart(fig3, use_container_width=True)

# ë„ì›€ë§ ë° íŒ
with st.expander("ğŸ’¡ ì‚¬ìš© íŒ & ë„ì›€ë§"):
    st.markdown("""
    ### ğŸ¯ íš¨ê³¼ì ì¸ í‰ê°€ë¥¼ ìœ„í•œ íŒ
    
    **1. ë°ì´í„°ì…‹ ì¤€ë¹„**
    - ì§ˆë¬¸ì— ì„ íƒì§€(A, B, C, D)ë¥¼ í¬í•¨ì‹œí‚¤ì„¸ìš”
    - ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”
    - ì •ë‹µì€ ë°˜ë“œì‹œ A, B, C, D ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•˜ì„¸ìš”
    
    **2. ëª¨ë¸ ì„ íƒ**
    - **ì˜ë£Œ íŠ¹í™”**: `microsoft/BioGPT` (ì¶”ì²œ)
    - **ë²”ìš© ìš°ìˆ˜**: `google/gemma-2b-it` (ì¶”ì²œ)
    - **ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: `distilgpt2`, `gpt2`
    
    **3. í‰ê°€ ê²°ê³¼ í•´ì„**
    - **80% ì´ìƒ**: ìš°ìˆ˜í•œ ì˜ë£Œ ì§€ì‹
    - **60-80%**: ì–‘í˜¸í•œ ì„±ëŠ¥
    - **40-60%**: ê¸°ë³¸ì ì¸ ì´í•´ ìˆ˜ì¤€
    - **40% ë¯¸ë§Œ**: ì¶”ê°€ í•™ìŠµ í•„ìš”
    
    **4. ë¬¸ì œ í•´ê²°**
    - API ì˜¤ë¥˜ ì‹œ: ì ì‹œ ê¸°ë‹¤ë¦° í›„ ì¬ì‹œë„
    - ëª¨ë¸ ë¡œë”© ì¤‘: 30ì´ˆ ì •ë„ ëŒ€ê¸° í•„ìš”
    - ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨: ì§ˆë¬¸ í˜•ì‹ í™•ì¸
    
    ### ğŸ“„ CSV í˜•ì‹ ì˜ˆì‹œ
    ```csv
    question,answer
    "í™˜ìì˜ ì¦ìƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤...
    
    A. ì§„ë‹¨ 1
    B. ì§„ë‹¨ 2  
    C. ì§„ë‹¨ 3
    D. ì§„ë‹¨ 4","A"
    ```
    """)

# í‘¸í„°
st.divider()
st.markdown("""
<div style='text-align: center; color: gray; padding: 2rem;'>
    <h4>ğŸ¥ MedLLM Leaderboard (Simplified)</h4>
    <p><strong>ê°„ì†Œí™”ëœ Question + Answer í˜•ì‹ìœ¼ë¡œ ì˜ë£Œ AI ëª¨ë¸ í‰ê°€</strong></p>
    <p>ğŸ“Š ë” ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ë°ì´í„° êµ¬ì¡° | ğŸš€ ë¹ ë¥¸ í‰ê°€ ë° ë¹„êµ</p>
    <p><em>â€» ì—°êµ¬ ë° êµìœ¡ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì‹¤ì œ ì˜ë£Œ ì§„ë‹¨ì—ëŠ” ì‚¬ìš© ê¸ˆì§€.</em></p>
</div>
""", unsafe_allow_html=True)