import streamlit as st
import pandas as pd
import requests
import json
import time
import re
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from io import StringIO

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¯ Simple Classification Leaderboard",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        margin: -1rem -1rem 2rem -1rem;
        border-radius: 0 0 20px 20px;
    }
    
    .metric-card {
        background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #3b82f6;
        margin: 0.5rem 0;
    }
    
    .rank-1 {
        background: linear-gradient(135deg, #ffd700 0%, #ffed4a 100%);
        color: #92400e;
        padding: 0.5rem;
        border-radius: 50%;
        text-align: center;
        font-weight: bold;
    }
    
    .rank-2 {
        background: linear-gradient(135deg, #c0c0c0 0%, #e5e7eb 100%);
        color: #374151;
        padding: 0.5rem;
        border-radius: 50%;
        text-align: center;
        font-weight: bold;
    }
    
    .rank-3 {
        background: linear-gradient(135deg, #cd7f32 0%, #d97706 100%);
        color: white;
        padding: 0.5rem;
        border-radius: 50%;
        text-align: center;
        font-weight: bold;
    }
    
    .stAlert > div {
        border-radius: 10px;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'leaderboard_data' not in st.session_state:
    st.session_state.leaderboard_data = []
if 'current_dataset' not in st.session_state:
    st.session_state.current_dataset = None
if 'api_key' not in st.session_state:
    st.session_state.api_key = ""
if 'models_to_evaluate' not in st.session_state:
    st.session_state.models_to_evaluate = []

# HuggingFace API í´ë˜ìŠ¤ (ë¶„ë¥˜ ë¬¸ì œìš©)
class HuggingFaceClassifier:
    def __init__(self, api_key):
        self.api_key = api_key
        self.api_url = 'https://api-inference.huggingface.co/models/'
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json',
        }
    
    def test_api_connection(self):
        """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.post(
                f"{self.api_url}distilbert-base-uncased",
                headers=self.headers,
                json={"inputs": "Hello"},
                timeout=10
            )
            return response.status_code in [200, 503]  # 503ì€ ëª¨ë¸ ë¡œë”© ì¤‘
        except:
            return False
    
    def query_model(self, model_name, inputs, max_retries=3):
        """ëª¨ë¸ ì¿¼ë¦¬ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(max_retries):
            try:
                payload = {
                    "inputs": inputs,
                    "parameters": {
                        "max_new_tokens": 50,
                        "temperature": 0.1,
                        "do_sample": False,
                        "return_full_text": False
                    },
                    "options": {
                        "wait_for_model": True
                    }
                }
                
                response = requests.post(
                    f"{self.api_url}{model_name}",
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 503:
                    if attempt < max_retries - 1:
                        st.info(f"ëª¨ë¸ ë¡œë”© ì¤‘... ({attempt + 1}/{max_retries})")
                        time.sleep(20)
                        continue
                    return {"error": "Model is still loading"}
                else:
                    return {"error": f"API Error: {response.status_code} - {response.text}"}
                    
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    st.warning(f"íƒ€ì„ì•„ì›ƒ... ì¬ì‹œë„ ì¤‘ ({attempt + 1}/{max_retries})")
                    time.sleep(10)
                    continue
                return {"error": "Request timeout"}
            except Exception as e:
                return {"error": str(e)}
        
        return {"error": "Max retries exceeded"}
    
    def get_model_info(self, model_name):
        """ëª¨ë¸ ì •ë³´ í™•ì¸"""
        try:
            response = requests.get(
                f"https://huggingface.co/api/models/{model_name}",
                timeout=10
            )
            if response.status_code == 200:
                return response.json()
            return None
        except:
            return None
    
    def classify_text(self, model_name, text, classes, progress_callback=None):
        """í…ìŠ¤íŠ¸ ë¶„ë¥˜ ìˆ˜í–‰"""
        results = []
        total_texts = len(text)
        
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = self.get_model_info(model_name)
        if not model_info:
            return [{"error": f"Model {model_name} not found or not accessible"}]
        
        for i, (txt, correct_class) in enumerate(zip(text, classes)):
            if progress_callback:
                progress_callback(i + 1, total_texts)
            
            # ë¶„ë¥˜ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""Classify the following text into one of these categories: {', '.join(set(classes))}

Text: "{txt}"

Classification:"""
            
            # API í˜¸ì¶œ
            response = self.query_model(model_name, prompt)
            
            predicted_class = "Unknown"
            error_msg = None
            
            if isinstance(response, dict) and "error" in response:
                error_msg = response["error"]
                predicted_class = "ERROR"
            elif isinstance(response, list) and len(response) > 0:
                if 'generated_text' in response[0]:
                    generated_text = response[0]['generated_text'].strip()
                    
                    # ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ì¤‘ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸°
                    unique_classes = list(set(classes))
                    for cls in unique_classes:
                        if cls.lower() in generated_text.lower():
                            predicted_class = cls
                            break
                    
                    # ì—¬ì „íˆ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ì²« ë²ˆì§¸ ë‹¨ì–´ ì‚¬ìš©
                    if predicted_class == "Unknown" and generated_text:
                        first_word = generated_text.split()[0] if generated_text.split() else ""
                        # ê°€ì¥ ìœ ì‚¬í•œ í´ë˜ìŠ¤ ì°¾ê¸°
                        for cls in unique_classes:
                            if first_word.lower() in cls.lower() or cls.lower() in first_word.lower():
                                predicted_class = cls
                                break
            
            results.append({
                'text': txt,
                'true_class': correct_class,
                'predicted_class': predicted_class,
                'correct': predicted_class == correct_class,
                'raw_response': response,
                'error': error_msg
            })
            
            # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            time.sleep(3)
        
        return results
    
    def calculate_metrics(self, results):
        """ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        valid_results = [r for r in results if not r.get('error')]
        correct = sum(1 for r in valid_results if r['correct'])
        total = len(valid_results)
        
        accuracy = correct / total if total > 0 else 0
        
        # í´ë˜ìŠ¤ë³„ ì •ë°€ë„, ì¬í˜„ìœ¨ ê³„ì‚°
        classes = list(set(r['true_class'] for r in valid_results))
        precision_sum = 0
        recall_sum = 0
        
        for cls in classes:
            tp = sum(1 for r in valid_results if r['true_class'] == cls and r['predicted_class'] == cls)
            fp = sum(1 for r in valid_results if r['true_class'] != cls and r['predicted_class'] == cls)
            fn = sum(1 for r in valid_results if r['true_class'] == cls and r['predicted_class'] != cls)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            
            precision_sum += precision
            recall_sum += recall
        
        avg_precision = precision_sum / len(classes) if classes else 0
        avg_recall = recall_sum / len(classes) if classes else 0
        f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0
        
        return {
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': avg_precision,
            'recall': avg_recall,
            'total_samples': total,
            'correct_predictions': correct,
            'score': accuracy
        }

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ¯ Simple Classification Leaderboard</h1>
    <p>ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œë¡œ AI ëª¨ë¸ ì„±ëŠ¥ í‰ê°€</p>
</div>
""", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°” - ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # API í‚¤ ì„¤ì •
    st.subheader("ğŸ”‘ HuggingFace API Key")
    api_key = st.text_input(
        "API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        type="password",
        value=st.session_state.api_key,
        help="https://huggingface.co/settings/tokens ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”"
    )
    
    if api_key != st.session_state.api_key:
        st.session_state.api_key = api_key
        if api_key:
            classifier = HuggingFaceClassifier(api_key)
            if classifier.test_api_connection():
                st.success("âœ… API Keyê°€ ì„¤ì •ë˜ê³  ì—°ê²°ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.error("âŒ API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        else:
            st.warning("âš ï¸ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    st.divider()
    
    # ë°ì´í„°ì…‹ ì—…ë¡œë“œ
    st.subheader("ğŸ“Š ë°ì´í„°ì…‹ ì—…ë¡œë“œ")
    
    # ìƒ˜í”Œ CSV ìƒì„±
    sample_csv = """text,label
"I love this movie! It's amazing!",positive
"This movie is terrible and boring.",negative
"The weather is sunny today.",neutral
"I hate waiting in long lines.",negative
"This book is fantastic and well-written.",positive
"The service was okay, nothing special.",neutral
"I'm so excited about the concert!",positive
"The food was cold and tasteless.",negative
"It's a normal day at work.",neutral
"This product exceeded my expectations!",positive
"I'm disappointed with the quality.",negative
"The presentation was informative.",neutral
"I can't wait for the weekend!",positive
"The traffic is really bad today.",negative
"The weather is nice for a walk.",positive
"""
    
    st.download_button(
        label="ğŸ“¥ ê°ì • ë¶„ë¥˜ ìƒ˜í”Œ CSV ë‹¤ìš´ë¡œë“œ",
        data=sample_csv,
        file_name="sentiment_classification_sample.csv",
        mime="text/csv"
    )
    
    uploaded_file = st.file_uploader(
        "CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:",
        type=['csv'],
        help="text, label ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤ (ê°ì •ë¶„ë¥˜, ì£¼ì œë¶„ë¥˜ ë“±)"
    )
    
    if uploaded_file is not None:
        try:
            stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
            df = pd.read_csv(stringio)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['text', 'label']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                st.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}")
            else:
                # ë¹ˆ ë°ì´í„° í•„í„°ë§
                df_clean = df.dropna(subset=required_columns)
                
                if len(df_clean) == 0:
                    st.error("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.session_state.current_dataset = df_clean
                    st.success(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ! ({len(df_clean)}ê°œ ìƒ˜í”Œ)")
                    
                    # í´ë˜ìŠ¤ ë¶„í¬ í‘œì‹œ
                    class_counts = df_clean['label'].value_counts()
                    st.write("**í´ë˜ìŠ¤ ë¶„í¬:**")
                    for cls, count in class_counts.items():
                        st.write(f"- {cls}: {count}ê°œ")
                    
                    if len(df_clean) < len(df):
                        st.warning(f"âš ï¸ {len(df) - len(df_clean)}ê°œì˜ ë¶ˆì™„ì „í•œ í–‰ì´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
    
    st.divider()
    
    # ëª¨ë¸ ì¶”ê°€
    st.subheader("ğŸ¤– ëª¨ë¸ ì¶”ê°€")
    
    # ì¶”ì²œ ëª¨ë¸ë“¤
    if st.session_state.api_key:
        with st.expander("ğŸŒŸ ì¶”ì²œ ë¶„ë¥˜ ëª¨ë¸"):
            recommended_models = [
                ("google/gemma-2b-it", "Gemma 2B Instruct"),
                ("microsoft/DialoGPT-medium", "DialoGPT Medium"),
                ("distilgpt2", "DistilGPT-2"),
                ("gpt2", "GPT-2"),
                ("microsoft/DialoGPT-small", "DialoGPT Small"),
                ("EleutherAI/gpt-neo-125M", "GPT-Neo 125M")
            ]
            
            for model_id, description in recommended_models:
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.write(f"**{description}**")
                    st.caption(model_id)
                with col2:
                    if st.button("â•", key=f"rec_{model_id}"):
                        if model_id not in [m['id'] for m in st.session_state.models_to_evaluate]:
                            st.session_state.models_to_evaluate.append({
                                'id': model_id,
                                'name': description
                            })
                            st.success(f"âœ… {description} ì¶”ê°€ë¨!")
                            st.rerun()
                        else:
                            st.warning("ì´ë¯¸ ì¶”ê°€ëœ ëª¨ë¸ì…ë‹ˆë‹¤")
    
    with st.form("add_model_form"):
        model_id = st.text_input(
            "HuggingFace Model ID:",
            placeholder="google/gemma-2b-it",
            help="ì˜ˆ: google/gemma-2b-it"
        )
        
        display_name = st.text_input(
            "Display Name (ì„ íƒì‚¬í•­):",
            placeholder="ì‚¬ìš©ì ì •ì˜ ì´ë¦„"
        )
        
        submitted = st.form_submit_button("â• ëª¨ë¸ ì¶”ê°€")
        
        if submitted and model_id:
            if '/' not in model_id and not model_id in ['gpt2', 'distilgpt2']:
                st.error("âŒ ì˜¬ë°”ë¥¸ ëª¨ë¸ ID í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. (ì˜ˆ: organization/model-name)")
            else:
                if not display_name:
                    display_name = model_id.split('/')[-1].replace('-', ' ').title()
                
                existing_ids = [m['id'] for m in st.session_state.models_to_evaluate]
                if model_id in existing_ids:
                    st.error("âŒ ì´ë¯¸ ì¶”ê°€ëœ ëª¨ë¸ì…ë‹ˆë‹¤.")
                else:
                    st.session_state.models_to_evaluate.append({
                        'id': model_id,
                        'name': display_name
                    })
                    st.success(f"âœ… {display_name} ëª¨ë¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
    
    # ì¶”ê°€ëœ ëª¨ë¸ ëª©ë¡
    if st.session_state.models_to_evaluate:
        st.subheader("ğŸ“ í‰ê°€ ëŒ€ê¸° ëª¨ë¸")
        for i, model in enumerate(st.session_state.models_to_evaluate):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.text(f"â€¢ {model['name']}")
                st.caption(model['id'])
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"remove_{i}"):
                    st.session_state.models_to_evaluate.pop(i)
                    st.rerun()

# ë©”ì¸ ì»¨í…ì¸ 
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ğŸ† ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬ë”ë³´ë“œ")
    
    if not st.session_state.leaderboard_data:
        st.info("ğŸ“Š ì•„ì§ í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì¶”ê°€í•˜ê³  í‰ê°€ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!")
    else:
        # ë¦¬ë”ë³´ë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df_leaderboard = pd.DataFrame(st.session_state.leaderboard_data)
        df_leaderboard = df_leaderboard.sort_values('score', ascending=False).reset_index(drop=True)
        
        # ë¦¬ë”ë³´ë“œ í…Œì´ë¸”
        for idx, row in df_leaderboard.iterrows():
            rank = idx + 1
            
            # ë­í¬ ì•„ì´ì½˜
            if rank == 1:
                rank_badge = "ğŸ¥‡"
            elif rank == 2:
                rank_badge = "ğŸ¥ˆ"
            elif rank == 3:
                rank_badge = "ğŸ¥‰"
            else:
                rank_badge = f"#{rank}"
            
            with st.container():
                col_rank, col_model, col_metrics, col_actions = st.columns([1, 3, 4, 1])
                
                with col_rank:
                    st.markdown(f"<h3 style='text-align: center;'>{rank_badge}</h3>", unsafe_allow_html=True)
                
                with col_model:
                    st.subheader(row['modelName'])
                    st.caption(row['modelId'])
                    st.caption(f"í‰ê°€ì¼: {row['evaluatedAt']}")
                
                with col_metrics:
                    metric_col1, metric_col2 = st.columns(2)
                    with metric_col1:
                        st.metric("ì •í™•ë„", f"{row['accuracy']:.1%}")
                        st.metric("F1 Score", f"{row['f1_score']:.1%}")
                    with metric_col2:
                        st.metric("ì •ë°€ë„", f"{row['precision']:.1%}")
                        st.metric("ì¬í˜„ìœ¨", f"{row['recall']:.1%}")
                
                with col_actions:
                    if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"delete_{idx}"):
                        st.session_state.leaderboard_data = [
                            item for item in st.session_state.leaderboard_data 
                            if item['modelId'] != row['modelId']
                        ]
                        st.rerun()
                
                st.divider()

with col2:
    st.header("ğŸ“Š í˜„ì¬ ë°ì´í„°ì…‹")
    
    if st.session_state.current_dataset is not None:
        st.success(f"âœ… {len(st.session_state.current_dataset)}ê°œ ìƒ˜í”Œ ë¡œë“œë¨")
        
        # ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
            st.dataframe(st.session_state.current_dataset.head(10))
    else:
        st.warning("âš ï¸ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
    
    st.divider()
    
    # í‰ê°€ ì‹¤í–‰
    st.header("ğŸš€ í‰ê°€ ì‹¤í–‰")
    
    can_run = (
        st.session_state.api_key and 
        st.session_state.current_dataset is not None and 
        st.session_state.models_to_evaluate
    )
    
    if not can_run:
        missing = []
        if not st.session_state.api_key:
            missing.append("API Key")
        if st.session_state.current_dataset is None:
            missing.append("ë°ì´í„°ì…‹")
        if not st.session_state.models_to_evaluate:
            missing.append("í‰ê°€í•  ëª¨ë¸")
        
        st.error(f"âŒ ë‹¤ìŒì´ í•„ìš”í•©ë‹ˆë‹¤: {', '.join(missing)}")
    
    if st.button("ğŸ¯ ë¶„ë¥˜ í‰ê°€ ì‹œì‘", disabled=not can_run, type="primary"):
        if can_run:
            classifier = HuggingFaceClassifier(st.session_state.api_key)
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ë°ì´í„° ì¤€ë¹„
            texts = st.session_state.current_dataset['text'].tolist()
            labels = st.session_state.current_dataset['label'].tolist()
            
            total_models = len(st.session_state.models_to_evaluate)
            
            try:
                for model_idx, model in enumerate(st.session_state.models_to_evaluate):
                    status_text.text(f"í‰ê°€ ì¤‘: {model['name']} ({model_idx + 1}/{total_models})")
                    
                    # ëª¨ë¸ ì •ë³´ í™•ì¸
                    model_info = classifier.get_model_info(model['id'])
                    
                    if not model_info:
                        st.error(f"âŒ ëª¨ë¸ '{model['id']}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    
                    pipeline_tag = model_info.get('pipeline_tag', 'unknown')
                    st.info(f"ğŸ“‹ ëª¨ë¸ ì •ë³´: {model['name']} (Pipeline: {pipeline_tag})")
                    
                    def progress_callback(current, total):
                        overall_progress = (model_idx + current/total) / total_models
                        progress_bar.progress(overall_progress)
                        status_text.text(f"í‰ê°€ ì¤‘: {model['name']} - {current}/{total} ìƒ˜í”Œ ì²˜ë¦¬ ì¤‘")
                    
                    # ë¶„ë¥˜ í‰ê°€ ì‹¤í–‰
                    results = classifier.classify_text(
                        model['id'],
                        texts,
                        labels,
                        progress_callback
                    )
                    
                    # ì˜¤ë¥˜ í™•ì¸
                    error_count = sum(1 for r in results if r.get('error'))
                    if error_count > 0:
                        st.warning(f"âš ï¸ {error_count}ê°œ ìƒ˜í”Œì—ì„œ ì˜¤ë¥˜ ë°œìƒ")
                    
                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    metrics = classifier.calculate_metrics(results)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.success(f"âœ… {model['name']} í‰ê°€ ì™„ë£Œ: {metrics['accuracy']:.1%} ì •í™•ë„")
                    
                    # ê²°ê³¼ ì €ì¥
                    new_entry = {
                        'modelId': model['id'],
                        'modelName': model['name'],
                        'accuracy': metrics['accuracy'],
                        'f1_score': metrics['f1_score'],
                        'precision': metrics['precision'],
                        'recall': metrics['recall'],
                        'score': metrics['score'],
                        'total_samples': metrics['total_samples'],
                        'correct_predictions': metrics['correct_predictions'],
                        'evaluatedAt': datetime.now().strftime('%Y-%m-%d'),
                        'pipeline_tag': pipeline_tag
                    }
                    
                    # ê¸°ì¡´ ê²°ê³¼ ì œê±° í›„ ìƒˆ ê²°ê³¼ ì¶”ê°€
                    st.session_state.leaderboard_data = [
                        item for item in st.session_state.leaderboard_data 
                        if item['modelId'] != model['id']
                    ]
                    st.session_state.leaderboard_data.append(new_entry)
                
                # í‰ê°€ ì™„ë£Œ
                st.session_state.models_to_evaluate = []
                progress_bar.progress(1.0)
                status_text.text("âœ… ëª¨ë“  ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
                st.success(f"ğŸ‰ {total_models}ê°œ ëª¨ë¸ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.balloons()
                st.rerun()
                
            except Exception as e:
                st.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                status_text.text("âŒ í‰ê°€ ì‹¤íŒ¨")

# ì„±ëŠ¥ ì°¨íŠ¸
if st.session_state.leaderboard_data:
    st.header("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸")
    
    df_chart = pd.DataFrame(st.session_state.leaderboard_data)
    
    # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
    fig = px.bar(
        df_chart.sort_values('score', ascending=True),
        x='score',
        y='modelName',
        orientation='h',
        title="ëª¨ë¸ë³„ ë¶„ë¥˜ ì •í™•ë„ ë¹„êµ",
        labels={'score': 'ì •í™•ë„', 'modelName': 'ëª¨ë¸ëª…'},
        color='score',
        color_continuous_scale='viridis'
    )
    
    fig.update_layout(height=400)
    st.plotly_chart(fig, use_container_width=True)
    
    # ë©”íŠ¸ë¦­ ë¹„êµ
    metrics_df = df_chart[['modelName', 'accuracy', 'f1_score', 'precision', 'recall']].melt(
        id_vars=['modelName'],
        var_name='metric',
        value_name='value'
    )
    
    fig2 = px.line(
        metrics_df,
        x='modelName',
        y='value',
        color='metric',
        title="ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ",
        markers=True
    )
    
    fig2.update_layout(height=400)
    fig2.update_xaxis(tickangle=45)
    st.plotly_chart(fig2, use_container_width=True)

# í‘¸í„°
st.divider()
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>ğŸ¯ Simple Classification Leaderboard</p>
    <p>í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œë¡œ AI ëª¨ë¸ ì„±ëŠ¥ì„ ê°„ë‹¨í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤</p>
</div>
""", unsafe_allow_html=True)