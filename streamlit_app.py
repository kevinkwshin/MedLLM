import streamlit as st
import pandas as pd
import re
import time
from io import StringIO
from openai import OpenAI
from datetime import datetime
import json

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ğŸ¥ MedLLM Benchmark",
    page_icon="ğŸ¥",
    layout="wide",
)

# --- CSS ìŠ¤íƒ€ì¼ë§ ---
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .stButton>button {
        width: 100%;
    }
    .benchmark-card {
        border: 1px solid #ddd;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        background: #f8f9fa;
        color: #333;
    }
    .benchmark-card h4 {
        color: #2c3e50;
        margin-bottom: 0.5rem;
    }
    .benchmark-card p {
        color: #555;
        margin: 0.3rem 0;
    }
    .accuracy-high { color: #28a745; font-weight: bold; }
    .accuracy-medium { color: #ffc107; font-weight: bold; }
    .accuracy-low { color: #dc3545; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if 'benchmark_results' not in st.session_state:
    # ìƒ˜í”Œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìƒì„±
    st.session_state.benchmark_results = [
        {
            'id': 1,
            'model_name': 'Qwen/Qwen3-32B',
            'accuracy': 87.5,
            'total_questions': 240,
            'correct_answers': 210,
            'evaluation_date': '2025-01-25 14:30:22',
            'dataset_name': 'Medical QA Dataset v1.2',
            'api_errors': 2
        },
        {
            'id': 2,
            'model_name': 'Qwen/Qwen3-1.7B',
            'accuracy': 82.3,
            'total_questions': 240,
            'correct_answers': 197,
            'evaluation_date': '2025-01-24 16:45:11',
            'dataset_name': 'Medical QA Dataset v1.2',
            'api_errors': 0
        },
        {
            'id': 3,
            'model_name': 'google/gemma-2-27b-it',
            'accuracy': 79.6,
            'total_questions': 240,
            'correct_answers': 191,
            'evaluation_date': '2025-01-23 09:15:33',
            'dataset_name': 'Medical QA Dataset v1.2',
            'api_errors': 1
        },
        {
            'id': 4,
            'model_name': 'microsoft/DialoGPT-medium',
            'accuracy': 71.2,
            'total_questions': 240,
            'correct_answers': 171,
            'evaluation_date': '2025-01-22 11:20:45',
            'dataset_name': 'Medical QA Dataset v1.1',
            'api_errors': 8
        },
        {
            'id': 5,
            'model_name': 'anthropic/claude-3-haiku',
            'accuracy': 91.7,
            'total_questions': 240,
            'correct_answers': 220,
            'evaluation_date': '2025-01-21 13:50:17',
            'dataset_name': 'Medical QA Dataset v1.2',
            'api_errors': 0
        }
    ]

if 'admin_mode' not in st.session_state:
    st.session_state.admin_mode = False

if 'admin_authenticated' not in st.session_state:
    st.session_state.admin_authenticated = False

# --- HuggingFace OpenAI í˜¸í™˜ API í´ë˜ìŠ¤ ---
class MedicalEvaluator:
    def __init__(self, api_key):
        if not api_key:
            raise ValueError("Hugging Face API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        self.client = OpenAI(
            base_url="https://router.huggingface.co/v1",
            api_key=api_key,
        )

    def query_model(self, model_name, prompt):
        try:
            completion = self.client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10,
                temperature=0.1,
            )
            return completion.choices[0].message.content
        except Exception as e:
            return f"API_ERROR: {str(e)}"

    def evaluate(self, model_name, questions, correct_answers, progress_bar):
        results = []
        total_questions = len(questions)
        api_errors = 0

        for i, (question, correct_answer) in enumerate(zip(questions, correct_answers)):
            prompt = f"""{question}\n\nAnswer with the letter of the correct option (A, B, C, or D).\n\nAnswer:"""
            
            response_text = self.query_model(model_name, prompt)
            
            predicted_answer = "EXTRACT_FAIL"
            error_message = None

            if response_text.startswith("API_ERROR:"):
                error_message = response_text
                predicted_answer = "API_ERROR"
                api_errors += 1
            else:
                match = re.search(r'\b([A-D])\b', response_text.upper())
                if match:
                    predicted_answer = match.group(1)

            is_correct = predicted_answer == correct_answer
            results.append({
                'question': question,
                'correct_answer': correct_answer,
                'predicted_answer': predicted_answer,
                'is_correct': is_correct,
                'raw_response': response_text,
                'error': error_message
            })
            
            progress_bar.progress((i + 1) / total_questions, text=f"Processing {i+1}/{total_questions}")
            time.sleep(0.2)

        return results, api_errors

def get_accuracy_class(accuracy):
    if accuracy >= 85:
        return "accuracy-high"
    elif accuracy >= 70:
        return "accuracy-medium"
    else:
        return "accuracy-low"

def add_benchmark_result(model_name, accuracy, total_questions, correct_answers, dataset_name, api_errors):
    new_id = max([r['id'] for r in st.session_state.benchmark_results]) + 1 if st.session_state.benchmark_results else 1
    new_result = {
        'id': new_id,
        'model_name': model_name,
        'accuracy': accuracy,
        'total_questions': total_questions,
        'correct_answers': correct_answers,
        'evaluation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'dataset_name': dataset_name,
        'api_errors': api_errors
    }
    st.session_state.benchmark_results.append(new_result)

def delete_benchmark_result(result_id):
    st.session_state.benchmark_results = [r for r in st.session_state.benchmark_results if r['id'] != result_id]

# --- ë©”ì¸ í—¤ë” ---
st.markdown('<div class="main-header"><h1>ğŸ¥ MedLLM Benchmark Results</h1></div>', unsafe_allow_html=True)

# --- ê´€ë¦¬ì ëª¨ë“œ í† ê¸€ (ìˆ¨ê¹€) ---
col1, col2, col3 = st.columns([6, 1, 1])
with col3:
    if st.button("ğŸ”§", help="Admin Mode"):
        if st.session_state.admin_authenticated:
            # ì´ë¯¸ ì¸ì¦ëœ ê²½ìš° ëª¨ë“œ í† ê¸€
            st.session_state.admin_mode = not st.session_state.admin_mode
            if not st.session_state.admin_mode:
                st.session_state.admin_authenticated = False
        else:
            # ì¸ì¦ì´ í•„ìš”í•œ ê²½ìš°
            st.session_state.admin_mode = True

# --- ê´€ë¦¬ì ì¸ì¦ ì„¹ì…˜ ---
if st.session_state.admin_mode and not st.session_state.admin_authenticated:
    st.markdown("---")
    st.header("ğŸ” Admin Authentication Required")
    
    col_auth1, col_auth2, col_auth3 = st.columns([1, 2, 1])
    with col_auth2:
        password = st.text_input("Enter Admin Password:", type="password", key="admin_password")
        
        col_login, col_cancel = st.columns(2)
        with col_login:
            if st.button("ğŸ”‘ Login", key="admin_login", use_container_width=True):
                if password == "passpass":
                    st.session_state.admin_authenticated = True
                    st.success("âœ… Admin ì¸ì¦ ì„±ê³µ!")
                    st.rerun()
                else:
                    st.error("âŒ ì˜ëª»ëœ ë¹„ë°€ë²ˆí˜¸ì…ë‹ˆë‹¤.")
        
        with col_cancel:
            if st.button("âŒ Cancel", key="admin_cancel", use_container_width=True):
                st.session_state.admin_mode = False
                st.rerun()

# --- ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í‘œì‹œ ---
st.header("ğŸ“Š Current Benchmark Rankings")

# ì •í™•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
sorted_results = sorted(st.session_state.benchmark_results, key=lambda x: x['accuracy'], reverse=True)

for i, result in enumerate(sorted_results):
    accuracy_class = get_accuracy_class(result['accuracy'])
    
    col1, col2, col3, col4 = st.columns([1, 4, 2, 1])
    
    with col1:
        st.markdown(f"<h2>#{i+1}</h2>", unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="benchmark-card">
            <h4>{result['model_name']}</h4>
            <p><strong>Dataset:</strong> {result['dataset_name']}</p>
            <p><strong>Date:</strong> {result['evaluation_date']}</p>
            {f"<p><strong>API Errors:</strong> {result['api_errors']}</p>" if result['api_errors'] > 0 else ""}
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div style="text-align: center;">
            <h2 class="{accuracy_class}">{result['accuracy']:.1f}%</h2>
            <p>{result['correct_answers']} / {result['total_questions']}</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        if st.session_state.admin_mode and st.session_state.admin_authenticated:
            if st.button("ğŸ—‘ï¸", key=f"delete_{result['id']}", help="Delete"):
                delete_benchmark_result(result['id'])
                st.rerun()

# --- ê´€ë¦¬ì ëª¨ë“œ: ìƒˆë¡œìš´ í‰ê°€ ì‹¤í–‰ ---
if st.session_state.admin_mode and st.session_state.admin_authenticated:
    st.markdown("---")
    st.header("ğŸ”§ Admin: Run New Evaluation")

    with st.sidebar:
        st.header("âš™ï¸ Evaluation Settings")

        st.info("**ê¶Œì¥:** Streamlit Cloudì˜ Secretsì— `HF_TOKEN`ì„ ì„¤ì •í•˜ì„¸ìš”.")
        api_key = st.text_input(
            "Hugging Face Token (hf_...)", 
            type="password", 
            help="Hugging Face API í† í°ì„ ì…ë ¥í•˜ì„¸ìš”.",
            value=st.secrets.get("HF_TOKEN", "")
        )
        
        # ì—¬ëŸ¬ ëª¨ë¸ ì…ë ¥
        model_ids_input = st.text_area(
            "HuggingFace Model IDs (í•œ ì¤„ì— í•˜ë‚˜ì”©)", 
            value="Qwen/Qwen3-32BQwen/Qwen3-1.7B/Llama-3.1-7B-Instruct",
            help="í‰ê°€í•  ëª¨ë¸ë“¤ì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”.",
            height=150
        )

        dataset_name = st.text_input(
            "Dataset Name",
            value="Medical QA Dataset v1.3",
            help="ë°ì´í„°ì…‹ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”."
        )

        st.subheader("ğŸ“Š ë°ì´í„°ì…‹")
        uploaded_file = st.file_uploader(
            "CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['csv'],
            help="'question'ê³¼ 'answer' ì»¬ëŸ¼ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."
        )
        
        sample_csv = '''question,answer
"25ì„¸ ì—¬ì„±ì´ ì‹¬í•œ ë³µí†µìœ¼ë¡œ ì‘ê¸‰ì‹¤ì— ë‚´ì›í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥¸ìª½ í•˜ë³µë¶€ì— ì••í†µì´ ìˆê³ , ë°œì—´ê³¼ ì˜¤ì‹¬ì„ ë™ë°˜í•©ë‹ˆë‹¤. ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì§„ë‹¨ì€?\nA) ê¸‰ì„± ìœ„ì—¼\nB) ê¸‰ì„± ì¶©ìˆ˜ì—¼\nC) ìš”ë¡œê°ì—¼\nD) ê¸‰ì„± ë‹´ë‚­ì—¼","B"
"45ì„¸ ë‚¨ì„±ì´ ìš´ë™ í›„ ì‹¬í•œ í‰í†µì„ í˜¸ì†Œí•©ë‹ˆë‹¤. í‰í†µì€ ì™¼ìª½ íŒ”ë¡œ ë°©ì‚¬ë˜ë©°, ì‹ì€ë•€ì„ í˜ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ì‹œí–‰í•´ì•¼ í•  ê²€ì‚¬ëŠ”?\nA) í‰ë¶€ X-ray\nB) ì‹¬ì „ë„(ECG)\nC) ë³µë¶€ CT\nD) í˜ˆì•¡ê²€ì‚¬","B"
"60ì„¸ ì—¬ì„±ì´ 3ê°œì›”ê°„ì˜ ì²´ì¤‘ê°ì†Œì™€ ë³µë¶€íŒ½ë§Œì„ ì£¼ì†Œë¡œ ë‚´ì›í–ˆìŠµë‹ˆë‹¤. ë³µë¶€ ì´ˆìŒíŒŒì—ì„œ ë³µê°• ë‚´ ë‹¤ëŸ‰ì˜ ë³µìˆ˜ê°€ ê´€ì°°ë©ë‹ˆë‹¤. ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì›ì¸ì€?\nA) ì‹¬ë¶€ì „\nB) ê°„ê²½í™”\nC) ë³µë§‰ì—¼\nD) ë‚œì†Œì•”","D"
'''
        st.download_button(
            label="ğŸ“¥ ìƒ˜í”Œ CSV ë‹¤ìš´ë¡œë“œ",
            data=sample_csv,
            file_name="medical_qa_sample.csv",
            mime="text/csv"
        )

    if st.button("ğŸš€ Start Batch Evaluation"):
        if not api_key:
            st.error("âŒ Hugging Face í† í°ì„ ì…ë ¥í•˜ê±°ë‚˜ Secretsì— ì„¤ì •í•˜ì„¸ìš”.")
        elif not model_ids_input.strip():
            st.error("âŒ ëª¨ë¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        elif uploaded_file is None:
            st.error("âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        else:
            try:
                # ëª¨ë¸ IDs íŒŒì‹±
                model_ids = [mid.strip() for mid in model_ids_input.strip().split('\n') if mid.strip()]
                
                df = pd.read_csv(uploaded_file)
                if 'question' not in df.columns or 'answer' not in df.columns:
                    st.error("âŒ CSV íŒŒì¼ì— 'question'ê³¼ 'answer' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
                else:
                    st.info(f"âœ… {len(df)}ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ.")
                    st.info(f"ğŸ¤– {len(model_ids)}ê°œ ëª¨ë¸ ìˆœì°¨ í‰ê°€ ì‹œì‘...")

                    evaluator = MedicalEvaluator(api_key)
                    questions = df['question'].tolist()
                    correct_answers = df['answer'].tolist()

                    # ê° ëª¨ë¸ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ í‰ê°€
                    for model_idx, model_id in enumerate(model_ids):
                        clean_model_id = model_id.strip().strip('"\'')
                        
                        st.subheader(f"Evaluating Model {model_idx + 1}/{len(model_ids)}: {clean_model_id}")
                        
                        progress_bar = st.progress(0, text=f"í‰ê°€ ì‹œì‘: {clean_model_id}")
                        evaluation_results, api_errors = evaluator.evaluate(clean_model_id, questions, correct_answers, progress_bar)
                        progress_bar.empty()

                        results_df = pd.DataFrame(evaluation_results)
                        correct_count = results_df['is_correct'].sum()
                        total_count = len(results_df)
                        accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0

                        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ì— ì¶”ê°€
                        add_benchmark_result(
                            clean_model_id, 
                            accuracy, 
                            total_count, 
                            correct_count, 
                            dataset_name,
                            api_errors
                        )

                        st.success(f"âœ… {clean_model_id}: {accuracy:.2f}% ({correct_count}/{total_count})")
                        
                        if api_errors > 0:
                            st.warning(f"âš ï¸ API ì˜¤ë¥˜ {api_errors}ê°œ ë°œìƒ")

                    st.success("ğŸ‰ ëª¨ë“  ëª¨ë¸ í‰ê°€ ì™„ë£Œ! í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ë©ë‹ˆë‹¤.")
                    time.sleep(2)
                    st.rerun()

            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

else:
    st.info("ìƒˆë¡œìš´ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
